{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML5_Task3_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nctrM1QSiuz",
        "colab_type": "text"
      },
      "source": [
        "## *Applied Machine Learning*\n",
        "\n",
        "# *Assignment 5*\n",
        "\n",
        "## Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAN0eDycSy6E",
        "colab_type": "text"
      },
      "source": [
        "***Teammates:***\n",
        "\n",
        "Karthik Rajaraman Iyer\n",
        "kr2859@columbia.edu\n",
        "\n",
        "Anjani Prasad Atluri\n",
        "aa4462@columbia.edu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwLVzOUc8cUA",
        "colab_type": "code",
        "outputId": "7106336a-59e4-4e9d-c7e5-e0c0ac50c396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Importing the required library\n",
        "from PIL import Image\n",
        "import glob\n",
        "import keras\n",
        "from numpy import asarray\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.models import Sequential\n",
        "from matplotlib import pyplot\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Activation, Dense,Input, add, Add, ZeroPadding2D, BatchNormalization,AveragePooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "from  keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhtH2lkbxIPV",
        "colab_type": "code",
        "outputId": "f49b6a2f-5f4d-4e21-e966-b16432609196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#importing required libraries\n",
        "from google.colab import drive\n",
        "\n",
        "#mounting drive\n",
        "drive.mount(\"/content/gdrive\",force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWmOmkKd7VFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data and extracting it\n",
        "!unzip '/content/gdrive/My Drive/cell_images.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UhAl-EEFzgU",
        "colab_type": "text"
      },
      "source": [
        "The images were read in as smaller resolution (40x40). The reason we used this resolution is because the smallest image in the dataset has the resolution (40x55). We scaled down the images by 255 (value of black in grayscale)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fguWrLzM8P2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading the images into a list (grayscaled and scaled down by 255)\n",
        "image_list = []\n",
        "target= []\n",
        "\n",
        "for filename in glob.glob('/content/cell_images/Parasitized/*.png'): \n",
        "  #The parameter order is width x height\n",
        "    im=Image.open(filename).convert('L').resize((40,40)) \n",
        "    image_list.append(asarray(np.uint8(im))/255)\n",
        "    target.append(1)\n",
        "\n",
        "for filename in glob.glob('/content/cell_images/Uninfected/*.png'): \n",
        "    im=Image.open(filename).convert('L').resize((40,40)) \n",
        "    image_list.append(asarray(np.uint8(im))/255)\n",
        "    target.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE2clHtf6OAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the image list to array\n",
        "d=np.stack( image_list,axis=0 )\n",
        "t=np.asarray(target).reshape(-1,1)\n",
        "\n",
        "#shuffle the dataset (along the first dimension with the target labels)\n",
        "X, y = shuffle(d, t)\n",
        "\n",
        "img_rows=40\n",
        "img_cols=40\n",
        "\n",
        "#Re-shaping the array\n",
        "X_all = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "#Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPopz2g33si_",
        "colab_type": "text"
      },
      "source": [
        "Seeing if the data is not jumbled while re-shaping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U7MkUKzYayW",
        "colab_type": "code",
        "outputId": "3ffdf11f-91f7-4600-9cfe-96d3052f177f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "#Seeing if the data is not lost while re-shaping\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(X_all[45,:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdM0lEQVR4nO2deZBdZZnGn/cu3Z10ErIQQkhARIPAqMQZRCh1VBinELVwVygdRlFwYUZLZUDLcauZ0pkScZtRRIGMo+CCiuU6VIYqxhEDyr4oIAZIyEIgTZJe7vrNH/fG6c73vMk9fZe+3d/zq+rq7rffc8537r3vPbef8y4WQoAQYu6Tm+kFCCF6g4JdiERQsAuRCAp2IRJBwS5EIijYhUiEtoLdzE4zs9+b2QNmdlGnFiWE6Dw23fvsZpYHcB+AlwLYBOBmAGeGEO7xthmwwTCE4WkdT0zCLDY5rhOHz49sz1r8GPUth1pL+y1anm5/587lkW1o05izsgyQ8/UI9Xr7x5vFTGAU5VCiD1ihjf2eCOCBEMKDAGBmVwM4A4Ab7EMYxvPs1DYOKQDABgdjmxMQv7/w+Mh202supb6bqnsiGwvrlYUFdPujrjkvsj3jgtupbyZyrX8ArU+UiDF+E5urbAjr3b+18zF+FYBHJv2+qWkTQvQh7VzZW8LMzgVwLgAMIf5IKYToDe1c2TcDOHzS76ubtimEEL4aQjghhHBCEfHHTyFEb2hHoCugIdCdikaQ3wzgrBDC3d42i2xp0P/s7bP92mMi2ymr76O+hw4+GdmOG4rekwEAldDeB73bx46IbBvHl1Hf4Xw5suWMi2uHDuyKbM+c9wjxBD710b+JbIuu+jX1nYtsCOuxKzzRWYEuhFA1s/MB/AINHefy/QW6EGJmaeutPITwUwA/7dBahBBdRBl0QiSCgl2IRFCwC5EIXb/PLqaSP3YNtde+NB7Z9lQGqO9Zq2+ObIcVR6jvaD2+3XnPRHdynw4u7o5sKwf4uiohzs0bzpHsNwDLC7Eavyof32UAgDd+5OeR7YpDTo9sh37+V3T7uYyu7EIkgoJdiERQsAuRCAp2IRJBAl0Xqb/wOZFt14djsQkAzjr0zsg24aSvVuqx/YGJFXwNpCK9Fvh7/PxcnMI6Vo9FwryT1jpWa732oWhx2en2sIj6PlpeEtk2Fnid/HGDcSrw8998S2S78ZQ45RgARh6N13D0u26ivrMNXdmFSAQFuxCJoGAXIhEU7EIkgoJdiESQGt8Bxs84kdp3vy1W3t915I3U9+FS3ORhvFakvjmLG47UA284yXxLRM0HgCdCa51/C0RJB7jCzo7v4fmOhfiOALtLAAB7akOR7fmL7o9sr10apxwDwOPHxs00L/rim6jvmr8nKn0fT0XWlV2IRFCwC5EICnYhEqGt/9nNbCOA3QBqAKohhBM6sSghROfphED3khDCjg7sZ1aw+40nRbbxs3jN9tvX/G9ku2u0/Vry8SoXpxhVJzW2PbhwWCMiYd4R3XKI7YP5KvWdR9J4WT08AEzU47XdO35YZHvAeHrxUwbjl/Jlp3+N+p5beUdkO+bzvHNv9SHeDbeX6GO8EInQbrAHAP9lZr9tTn4RQvQp7X6Mf0EIYbOZHQLgOjP7XQjhhskOGv8kRH/Q1pU9hLC5+X07gB+gMdl1Xx+NfxKiD5h2sJvZsJkt3PszgL8GcFenFiaE6CztfIxfAeAHzbngBQDfCiHErT1nMXte/7zINnDO1sh25qpb6fZ3E+W9UucqMsNLga1kUNjLJDV2IMdVb+bL1lCt8+PXM6yLzXVbMhB32AWyPWbVQrwGT7lnsJTfstNE5KpXfzGyffRbb+U77gM1vp1Zbw8COL6DaxFCdBHdehMiERTsQiSCgl2IRFA9+35Y+K5Nke3UQ34X2e7Ys7rlfZYcsalVcQwAyrX2nrYJJ92VwbrTslTXrAzkYyHMq99neGnAzF4gYuCgI1KyOvkdlYXUd0P96ZHt0RfxDrmHbz40slW3xGJvN9GVXYhEULALkQgKdiESQcEuRCIo2IVIBKnx++Gx0bjbKlNmR51mEkzJnqhyxXl3JS4S8tT4kCGFdbgYN37wsAydYBmsO2zNWdc4eRzyOT5DbihfaXkNTNFndw+q+QzXOecmAUvD/dS7Lqe+Hxx+W2Q76goeftVH4rtAnUBXdiESQcEuRCIo2IVIBAW7EIkggW4/VGqxAMMEICbEAcAYEe5GK1zMK1Xjp6LgCFYVR/Ri7HGOx2BnMViI00q9LrBMjMsi+pWcNGAmVHqjolhN/VAhFvi8Y9F1OeOyFhXi+vubRp9GfT/x5m9Gtg8tPpP6HnNx/LxXNz+6vyW2hK7sQiSCgl2IRFCwC5EICnYhEuGAKoWZXQ7gFQC2hxCe2bQtBfBtAEcC2AjgDSGEnd1b5sxQLscPz45SPL+bCXEAUCYCH6utBoBChky3Kqm5ZscCgEo1tnuZeQVSY85kMG/7bpGl4SQTCctk+yxZebUcn0fPMujGavy1cMfYEZHtky//LvX9x2VnRLY1Z/dGoLsSwGn72C4CsD6EsAbA+ubvQog+5oDB3pzw8sQ+5jMArGv+vA7Aqzq8LiFEh5nuffYVIYQtzZ+3otFDnqLxT0L0B20LdCGEAP6v3d6/a/yTEH3AdIN9m5mtBIDm9+2dW5IQohtM92P8jwCcDeDTze/XdmxF+yPHVVkrktOocQU1C/V6rDoXiDLr1ZIzFTmLku2lhDLlfazEVeBqNV5bPs/vCFRrsW+OLDfLOeSdcygy5d/Zby1D/X6J2FjasZcuO0ae3/kFfqeEnduo8yF3cXEssmUZS9UJDnhlN7OrANwI4BlmtsnMzkEjyF9qZvcD+Kvm70KIPuaAV/YQAs/WB07t8FqEEF1EGXRCJIKCXYhEmFX17Fv/Lp6XDgB/fuadke2+keXUd6ISn3KpwjsKHrMsvsmwY4Kkyzo145t3LI5s3j1KJk3lC+2LjDWSLlslNm8NzHdwkKeaMsFqaID7BiIyerJfljaYrAcB6xUwQOr0AdCI8ObOs0ajw46YR7ev81vRodQd4U5XdiESQcEuRCIo2IVIBAW7EImgYBciEfpWjX/okydHtle+4kbqu2ow7ptx8MAe6jtSiSvvSk5zhD1kJFOZpDju2BOPiQKA+tahyJarcs25upCk4Ra4Dm0DsW+uyFNgs7SZYOmqTM0fJ2m1ADDgqPSMAknZ9TrRstTabsFSgVnzCwDIkUYkLLUXAMZJw5GNEwdT35c9J7679LOvnEh9j37nTdTO0JVdiERQsAuRCAp2IRJBwS5EIsy4QPfAZ0+i9te8+NeR7eAiF93uGzs0so2U51FfNqrJ68zK0iTZfPXxUZ4umy/Hx8qPOwLdIrL9MBe8hoZiO6u99/CEMDr3ndTD153Hq1QiM9ed2vkhIrp5s9yzjH9qVczzavLZa8Grsx8gte8TZDwYAFSZyOdM5jp6/tbIdu/T3M5vLaMruxCJoGAXIhEU7EIkgoJdiERopQfd5Wa23czummT7uJltNrPbml+nd3eZQoh2aUWNvxLAlwD8xz72S0IIn2l3Acef8AdqP2belsh2++jh1Jd1cZ1wuocyhZ0p9ADvYMpsIYMS7uWvGkmj9fY6fzBukJAnHVQB3lDCu/vAVOcya37hdO5ljS48dZytK+f40hRW0pCiE9SJyE9uqgDgdzXmFfgdlEUDE5HN63C7sxqnXx+18HHqe/0XpjZ0Kf1rfBdrL9Md/ySEmGW08z/7+WZ2R/Nj/pKOrUgI0RWmG+xfBvA0AGsBbAFwsedoZuea2W/M7DcV2sJfCNELphXsIYRtIYRaCKEO4DIAvP4OmvUmRL8wLZXDzFZOmuL6agB37c9/L7Wlw3jy9Knpsfnaw9T34fKyyDZa5W8W4yRFccAReyZI+qc7SogIKKNkzFIg+2z8oUUbgFyJ1ZI7Y6WIELZkwTj1PWgwtj82HnfIBXhnVp5ay182A6Qb7ryi08U1A1WyLiakAUCdPJesdj4LBee1ROv/ndcSS7POOS+Gx8vx83P08Dbqe9bLp/Z4eM+lj1E/oIVgb45/ejGAg81sE4CPAXixma1F46W7EcB5B9qPEGJmme74p693YS1CiC6iDDohEkHBLkQiKNiFSISeNq/I7xzD4u/dOsW26fVHUd9nLeZNAFrFbUhBklB9BTV+ePaMxh1jbcyZnUZE3JzTgJWly4YJvt/xofix2VngzTpYZ1SvGUOBpNxmmbNG1WnnWAzvecgCO16d3L3wFPZchsxndhdnyOkInIXRWnzHZ3t5IfX9Q2FqU4tS2OzuV1d2IRJBwS5EIijYhUgEBbsQidBTga5+0DyMveT4KbbVB22ivkWibnl154wqGdPk+7be1ZSJUCHvdGslu60NcV8m3BVG+NNTqsVi3E4iQgFAWBSv1xOnWI150amTZ7DHyxPdaJ27cyy2j7zzUsix100GzYz5ep1smaDJeit4+xjK8Hou1blgPbbPWKn9xYiu7EIkgoJdiERQsAuRCAp2IRJBwS5EIvRUjT/i8O34wiVfnGK7rcQ7xv6xtDyyecX+TE330mUZw8W4WysAlEi6bJ40aKiT9EYAYDcE2muj0CBXIg04dnO19slcnN677KBR6sseXZYS6um9THH2OsZmwUvvbZV8Ll6X142XKf/eXLhWtweAOsnD9V6j8/LxrZlBL886A7qyC5EICnYhEkHBLkQitDL+6XAzu97M7jGzu83svU37UjO7zszub35X73gh+phWBLoqgA+EEG4xs4UAfmtm1wH4WwDrQwifNrOLAFwE4ML97cgAFG2qMMLSYgFgwkkPZJSdMTqtb9+6mMe6rYYMNcxuFi/RgDxdKBDByXvbDkQwYiOdPFhnV9ZFFgAGC613kmV1552oZ2fprl5qLcMT7hi88y6HiYxeSjd7bCqO77720E66bAhhSwjhlubPuwHcC2AVgDMArGu6rQPwqgPtSwgxc2R6KzWzIwE8B8AGACsm9Y7fCmCFs5kQog9oOdjNbAGAawC8L4Swa/LfQggBTgejyeOfdj7RibvMQojp0FKwm1kRjUD/Zgjh+03zNjNb2fz7SgDb2baTxz8tWSrxX4iZopWJMIbGUIh7QwifnfSnHwE4G8Cnm9+vPdC+Htq0Aud94L1TbEe8/z7qu2ZB/N5RcerOcxZ/YvBqkLNkQy0YiAdR7h6IR1CV53Nhqk4y3azCj09r4r33RmZ3auqNCE6lChc/mThVrcUH8x5bs1hEml/kmV9MOPTq2dsd/8TyFoMjeGUR3VhD0kEnY5BleeYdQbJKauI9IXtfu+2nRWgrMvbzAbwFwJ1mdlvT9mE0gvw7ZnYOgIcAvKGFfQkhZohWxj/9En469KmdXY4Qolvon2ghEkHBLkQiKNiFSISe1rPndo5i+JoNU2wj5x9GfQ8p7opsj+YWU98yUUC9VESWGuvVSzMFdcFQrNCXyk4X2FJ8LCt7ea3E5CjsIOm5+UFHrS3GdtrZFfxORT4fK9neHY2JSvw4jJWcWn92vs5+syjkRnYxSFR+r8Mug3XdBXh3WVb/D/A7Hd7jWMjFa2M17gBT4310ZRciERTsQiSCgl2IRFCwC5EIPRXoGNu/9RRqv+q1caPEkw/5I/UdrXIRqFU8AYimKJJHbNki3sBxB6tLLvGHPFRbf99lYtzgEG+aOUzsnkBXIeJlhdS+Mz8AKE3Eabi1Cj8vdr5W5Omy84ZjUdQjR4QwVn/viW7M6smDTIzzHtsCSekeyvM06317PgDAeK31/g4eurILkQgKdiESQcEuRCIo2IVIBAW7EIkw42r8sstupPb7nnNiZFuw8nfUl6UXeqOiWErmngpX81nqIUuRHHAU2NXLRiLb7lLc/AIAavX4aLwRA2dogKdTes0UGOx8mbrM0mIBngJbyTlNIuKbLRgcdM6BqOleuitrrMHSUsedc2CjolyFPUMn2nmF+NyGiM3dvuV0WT+1WFd2IRJBwS5EIijYhUiEdsY/fdzMNpvZbc2v07u/XCHEdGln/BMAXBJC+ExXVlaMhYYhZ0Y1Sy9kHWcBXlfsCWwsLdSrV2YwqcQ7Fp3l7tQ7ZxmTNEY6yWaZd86EMDJqHAAXCb1RUezcPNGN2b0RVt7aWl0XS6P1hLgsAhtdQ46nyy4diNOvDy7upr5HFh+b8vug+SO4Wmk4uQXAlubPu81s7/gnIcQsop3xTwBwvpndYWaXa4qrEP1NO+OfvgzgaQDWonHlv9jZ7k/jnypovXpJCNFZpj3+KYSwLYRQCyHUAVwGIM6CwdTxT0XwhBIhRPdpRY2n45/2znlr8moAd3V+eUKITtHO+KczzWwtGqLzRgDndXJhNhCrpfNzvEEDS5H0FNRCaH0uXKvdQ1nHWg8v9dJLQWUwxdlT6Jny7s1JYzPV2Kw3d13k8fLSeNmxPDzlnZGlEy2Dbe3ts0Jmsg06DSmYcs/uIgHA/Hz8Ot9RWUh9P/nIK6f8/mhpHfUD2hv/9NMDbSuE6B+UQSdEIijYhUgEBbsQiTDj9ewe9nhcY/5waSn1XVQYj2xZOs6yzp8AUCezhJjA5klCLN3WS4FtNc3TI0sKrAdLS2U19UyIA7IJhwxPCMu3eWosRdpdQ4b9stfNAOmtAPB6dJYWCwCrBnZGtqs2PZf6Dp/24JTfc4GL2ICu7EIkg4JdiERQsAuRCAp2IRJBwS5EIvStGv/0D94c2X78kZOp73lvjJP5RirzqW89g95aLrc258ybfcYU8izqdBa8FNgsHWprZL20w67zELI1eEp6lrRWdgfEu6vB0nDZY56lY6ybAkvsXtMUhpemPVaP7yQ9Pspfzyup1TleBl8hxCxGwS5EIijYhUgEBbsQidC3Ah3qsYBiziQjVufupSLWy7Gw49WjM2GGim4ZUlU9gY4JVllSYD0hzkttZXjCW3SsDGJgFsHKrRsnz48rsBE7G4HlHYuJZp6QNkBeH0POmKaFhYnIdvTQVur7qVtPi33f/Ufq2/pwL13ZhUgGBbsQiaBgFyIRWmk4OWRmN5nZ7c3xT59o2p9qZhvM7AEz+7aZtV5TKoToOa0IdCUAp4QQ9jRbSv/SzH4G4P1ojH+62sy+AuAcNHrJd40jL/09tf/7rldFtnPe+RPqO1qL21nXiq0LYWx80+5y6y2yvWw7RpZGloU8F8KYEMVmwQO+8NbOGtwadSIcekIYy5bzMujY+CZ2rKJTd86eX090Y/bFxbi3AgAsK8aC8fLCLuoLJgKPPMl9M3DAK3tosKf5a7H5FQCcAuB7Tfs6AHHECSH6hlaHROSbbaS3A7gOwB8AjIQQ9t572ATNfxOir2kp2JuTX9YCWI3G5JdjWj2Axj8J0R9kUuNDCCMArgdwMoDFZrb3f/7VADY722j8kxB9QCtq/HIzW9z8eR6AlwK4F42gf13T7WwA13ZrkUKI9mlFjV8JYJ2Z5dF4c/hOCOHHZnYPgKvN7J8A3IrGPLiuUtvxOLUf9o27I9s3dp1Ofd/yD3Ht+8aJZdSXjecZrxUjGxsJBWSrXWcqvdcVlanWnsrPlHevky1Tzr201G7g3RPxVHoGq0enXWCd82IKu6fGLyjEadqLSFosALxgQXwn6e3r30Z9j/1crLx34lloZfzTHWjMZN/X/iCcya1CiP5DGXRCJIKCXYhEULALkQj9W8+eAZZKuOy7t1Pfb9ZeFtledsEN1HdBIc4LeHRicWQrOKmXeyrxrcY8EfgAPjaoGvh7sZcqysgTNc4TvGhzSSJ4Vbya/Ba3B7gYmEWI89Jd2T7YY8vmpQN8TFPOGfC1fGB3ZPuL+bzu/J3XnBvZjr1iB/Wt3Xs/tbeLruxCJIKCXYhEULALkQgKdiESQcEuRCLMCTWeUR8bo/aD/vPXke0n+RdR3+PffUdkWz20M7JtKy9qeV1emiZrVFENrTevKFX5U+kp+u3gpfGytNQsXVyz4Kn8TDlnXWCH83GqKwAsLsavG6bQA8Ahxbj5xAVXn019j/56XCdW3fgw9e0WurILkQgKdiESQcEuRCIo2IVIhDkr0GVhybobqf2WoXge/CNvjEWV16y8lW6/ubwksrF0WwCok0HmrHYe4DPmvZprVms/4eyX1d8zgc0TGZk45o1/YgKb1xeAzUFnj4G3hmFSd86EOABYUojtC/O8Rv3PBmPR7ak/5B1jey3GMXRlFyIRFOxCJIKCXYhEaGf805Vm9kczu635tbb7yxVCTJd2xj8BwAUhhO/tZ1shRJ/QSsPJAICNf5rzHHxprNLvqMUK/aefvZpuX1wRz/360PE/p77bKgdFtu3lhdR3V3VeZPMaWlDlvhar255vuwzk+LFY514Pti4vhZUxnI+bkMx30mUPImr8qmKcIg0Ab73hrZHt2CefoL6969HrM63xTyGEDc0//bOZ3WFml5iZJkAI0cdMa/yTmT0TwIfQGAP1XABLAVzIttX4JyH6g+mOfzothLClOeG1BOAKOD3kNf5JiP5guuOffmdmK5s2Q2Nc813dXKgQoj3aGf/032a2HI2morcBeGcX19k3LPtaLNrx4VFA/YXRIB186vzTqG+tGteuv+1Zv+JrKI5GtidrsWjn4Yl5pXr8cmA19ZU6r7NnHV+9zqyDuVhgK9V5Gi9bV5b9rhqMBbbDHNFtpDYc2d5/4xuo7zPeEXcwrlW5INkPtDP+6ZSurEgI0RWUQSdEIijYhUgEBbsQiaBgFyIRrJEN2xsW2dLwPDu1Z8ebTeTmz49sD3/jqdR3oBCr3mtXxI0UAOCIeXH6pqdk76oO7W+J/799ho6xnvI/SNJo804KLVvvEFHdAWB+Lk6DvXHkqMi2bZynIj/4yPLIdvRbf0t9+5ENYT12hSfog64ruxCJoGAXIhEU7EIkgoJdiERQd9k+gY2rWv3au1ve/obPnUTtf3lSvA9PoFu78JHINhHil8iQ8ZTQGqk7Z4IZAAzn4grIhbm4/h/gwt0vRp5FfZnwd9f3j41sKy/mqchHY+a7wHYLXdmFSAQFuxCJoGAXIhEU7EIkgoJdiESQGj9HWHMhnze3NR83mthyDm/x//UP/TKy3V2OFfLd9QG6PVPjl+e5wj6fzZAznlpbJPu9+MPHU9/BG+KGSSvLG4hneujKLkQiKNiFSAQFuxCJoGAXIhF6Ws9uZo8BeKj568EAdvTs4L1D5zX7mEvn9pQQQlyUjx4H+5QDm/0mhHDCjBy8i+i8Zh9z+dwmo4/xQiSCgl2IRJjJYP/qDB67m+i8Zh9z+dz+xIz9zy6E6C36GC9EIvQ82M3sNDP7vZk9YGYX9fr4ncTMLjez7WZ21yTbUjO7zszub35fMpNrnA5mdriZXW9m95jZ3Wb23qZ9Vp+bmQ2Z2U1mdnvzvD7RtD/VzDY0X5PfNjOe/D/L6WmwNyfB/huAlwE4DsCZZnZcL9fQYa4EsO9Y1osArA8hrAGwvvn7bKMK4AMhhOMAnATgPc3nabafWwnAKSGE4wGsBXCamZ0E4F8AXBJCeDqAnQDOmcE1do1eX9lPBPBACOHBEEIZwNUAzujxGjpGCOEGAPtOYTgDwLrmz+vQmF0/qwghbAkh3NL8eTeAewGswiw/t9BgT/PXYvMrADgFwPea9ll3Xq3S62BfBWByV8NNTdtcYkUIYUvz560AVszkYtrFzI5EY2T3BsyBczOzvJndBmA7gOsA/AHASAhhb6fKufiaBCCBrquExq2OWXu7w8wWALgGwPtCCLsm/222nlsIoRZCWAtgNRqfNI+Z4SX1jF4H+2YAh0/6fXXTNpfYZmYrAaD5ffsMr2damFkRjUD/Zgjh+03znDg3AAghjAC4HsDJABab2d5GLnPxNQmg98F+M4A1TfVzAMCbAPyox2voNj8CcHbz57MBXDuDa5kWZmYAvg7g3hDCZyf9aVafm5ktN7PFzZ/nAXgpGnrE9QBe13SbdefVKj1PqjGz0wF8DkAewOUhhH/u6QI6iJldBeDFaFRNbQPwMQA/BPAdAEegUeH3hhBCPEq1jzGzFwD4HwB3Atg7oeHDaPzfPmvPzcyejYYAl0fjQvedEMInzewoNMTipQBuBfDmEEI8xWKWoww6IRJBAp0QiaBgFyIRFOxCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIhP8D7UqRgbgmSh4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCca6Y7ENX_J",
        "colab_type": "text"
      },
      "source": [
        "We are not transforming the target variable into categorical variable as this is just a binary class classification. Only one binary class variable will be enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukhhYUDoTaQ",
        "colab_type": "text"
      },
      "source": [
        "###Task 3.1​ \n",
        "Start with a convolutional model without residual connections (using batch normalization is likely to be helpful and you should try it, whether you use dropout is your choice)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVuvd4croeSc",
        "colab_type": "code",
        "outputId": "9331c4e3-706e-4f14-b553-3959111b10eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(64, activation='relu'))\n",
        "cnn.add(Dense(1, activation='sigmoid'))\n",
        "cnn.compile(\"adam\", \"binary_crossentropy\", metrics=['accuracy'])\n",
        "history_cnn = cnn.fit(X_train, y_train,batch_size=256, epochs=20, verbose=1, validation_split=.2)\n",
        "cnn.evaluate(X_test, y_test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17636 samples, validate on 4410 samples\n",
            "Epoch 1/20\n",
            "17636/17636 [==============================] - 2s 120us/step - loss: 0.4516 - accuracy: 0.7811 - val_loss: 0.6805 - val_accuracy: 0.5639\n",
            "Epoch 2/20\n",
            "17636/17636 [==============================] - 1s 80us/step - loss: 0.2553 - accuracy: 0.8898 - val_loss: 0.6749 - val_accuracy: 0.5497\n",
            "Epoch 3/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.1755 - accuracy: 0.9315 - val_loss: 0.6427 - val_accuracy: 0.6626\n",
            "Epoch 4/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.1477 - accuracy: 0.9441 - val_loss: 0.6601 - val_accuracy: 0.5683\n",
            "Epoch 5/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.1275 - accuracy: 0.9521 - val_loss: 0.8129 - val_accuracy: 0.5277\n",
            "Epoch 6/20\n",
            "17636/17636 [==============================] - 1s 79us/step - loss: 0.1191 - accuracy: 0.9564 - val_loss: 0.6139 - val_accuracy: 0.6855\n",
            "Epoch 7/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.1107 - accuracy: 0.9591 - val_loss: 0.4883 - val_accuracy: 0.7707\n",
            "Epoch 8/20\n",
            "17636/17636 [==============================] - 1s 80us/step - loss: 0.0990 - accuracy: 0.9635 - val_loss: 0.3494 - val_accuracy: 0.8481\n",
            "Epoch 9/20\n",
            "17636/17636 [==============================] - 1s 80us/step - loss: 0.0891 - accuracy: 0.9685 - val_loss: 0.2740 - val_accuracy: 0.8916\n",
            "Epoch 10/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0781 - accuracy: 0.9732 - val_loss: 0.2196 - val_accuracy: 0.9163\n",
            "Epoch 11/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0733 - accuracy: 0.9739 - val_loss: 0.2534 - val_accuracy: 0.9043\n",
            "Epoch 12/20\n",
            "17636/17636 [==============================] - 1s 82us/step - loss: 0.0646 - accuracy: 0.9769 - val_loss: 0.2075 - val_accuracy: 0.9218\n",
            "Epoch 13/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0596 - accuracy: 0.9786 - val_loss: 0.3308 - val_accuracy: 0.9029\n",
            "Epoch 14/20\n",
            "17636/17636 [==============================] - 1s 80us/step - loss: 0.0525 - accuracy: 0.9820 - val_loss: 0.1968 - val_accuracy: 0.9322\n",
            "Epoch 15/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0396 - accuracy: 0.9878 - val_loss: 0.7266 - val_accuracy: 0.7354\n",
            "Epoch 16/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0418 - accuracy: 0.9862 - val_loss: 0.6745 - val_accuracy: 0.7512\n",
            "Epoch 17/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0362 - accuracy: 0.9884 - val_loss: 3.5009 - val_accuracy: 0.5179\n",
            "Epoch 18/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0410 - accuracy: 0.9858 - val_loss: 0.1924 - val_accuracy: 0.9429\n",
            "Epoch 19/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 0.2746 - val_accuracy: 0.9247\n",
            "Epoch 20/20\n",
            "17636/17636 [==============================] - 1s 81us/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.2487 - val_accuracy: 0.9361\n",
            "5512/5512 [==============================] - 0s 87us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.267684856897047, 0.9363207817077637]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_6uKe-b2lEI",
        "colab_type": "text"
      },
      "source": [
        "The accuracy on the test set it 93.63%. Having batch normalization improved the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx97SCZ7wYo8",
        "colab_type": "text"
      },
      "source": [
        "### Task 3.2 ​\n",
        "Augment the data using rotations, mirroring and possibly other transformations. How much can you improve your original model by data augmentation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2-dm2rKunQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "bfac3571-93cf-441e-e1ad-c536dcc0209e"
      },
      "source": [
        "from keras import backend as K \n",
        "import gc\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2)\n",
        "\n",
        "#splittng the data into train and validation set\n",
        "X_train_fin, X_val, y_train_fin, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1234)\n",
        "\n",
        "datagen.fit(X_train_fin)\n",
        "\n",
        "history_cnn1 = cnn.fit_generator(datagen.flow(X_train_fin, y_train_fin, batch_size=32),steps_per_epoch=len(X_train_fin) / 32, epochs=20)\n",
        "cnn.evaluate(X_test, y_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1987 - accuracy: 0.9223\n",
            "Epoch 2/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1887 - accuracy: 0.9253\n",
            "Epoch 3/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1932 - accuracy: 0.9252\n",
            "Epoch 4/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1944 - accuracy: 0.9241\n",
            "Epoch 5/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1861 - accuracy: 0.9276\n",
            "Epoch 6/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1901 - accuracy: 0.9261\n",
            "Epoch 7/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1869 - accuracy: 0.9271\n",
            "Epoch 8/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1939 - accuracy: 0.9247\n",
            "Epoch 9/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1814 - accuracy: 0.9297\n",
            "Epoch 10/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1832 - accuracy: 0.9290\n",
            "Epoch 11/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1931 - accuracy: 0.9245\n",
            "Epoch 12/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1895 - accuracy: 0.9266\n",
            "Epoch 13/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1877 - accuracy: 0.9276\n",
            "Epoch 14/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1880 - accuracy: 0.9276\n",
            "Epoch 15/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1876 - accuracy: 0.9273\n",
            "Epoch 16/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1823 - accuracy: 0.9283\n",
            "Epoch 17/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1842 - accuracy: 0.9296\n",
            "Epoch 18/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1751 - accuracy: 0.9318\n",
            "Epoch 19/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1811 - accuracy: 0.9302\n",
            "Epoch 20/20\n",
            "552/551 [==============================] - 8s 15ms/step - loss: 0.1816 - accuracy: 0.9286\n",
            "5512/5512 [==============================] - 1s 91us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15559678318733228, 0.944847583770752]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ReWa-l1GGJq",
        "colab_type": "text"
      },
      "source": [
        "We can see that the loss nearly halved and the accuracy increased after using the augmentation technique. This might be because the cells in the images are in different orientations (rotations, width and height shifts). We haven't used feature_wise_normalization as we have already scaled the data down to 255th of its original size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-MN38NDGtU8",
        "colab_type": "text"
      },
      "source": [
        "### Task 3.3​ \n",
        "\n",
        "Build a deeper model using residual connections. Show that you can build a deep model that would not be able to learn if you remove the residual connections (i.e. compare a deep model with and without residual connections while the rest of the architecture is constant). Feel free to reuse existing architectures from the literature or use them as inspiration for your own. You can find commonly used architectures here:\n",
        "https://keras.io/applications/\n",
        "However, the point of the exercise is to learn the weights from scratch, so please do not reuse the weights shipped with these applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAHA7KD49uxc",
        "colab_type": "text"
      },
      "source": [
        "Let us use resnet50 architecture for this part. First we will remove the skip connections from it and show that the model doesn't learn anything.\n",
        "\n",
        "The code for resnet was taken from https://datascience-enthusiast.com/DL/Residual_Networks_v2.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVSbdzSdgfT0",
        "colab_type": "code",
        "outputId": "e08fa3e3-b094-4b71-96d7-5482f9407c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone 'https://github.com/marcopeix/Deep_Learning_AI.git'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep_Learning_AI'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Total 311 (delta 0), reused 0 (delta 0), pack-reused 311\n",
            "Receiving objects: 100% (311/311), 42.05 MiB | 32.84 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EesbPqS0ijvV",
        "colab_type": "code",
        "outputId": "463665d9-28ca-40be-a9a7-1e67943a91fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd Deep_Learning_AI/'4.Convolutional Neural Networks'/'2.Deep Convolutional Models'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Deep_Learning_AI/4.Convolutional Neural Networks/2.Deep Convolutional Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOSxcT9piqCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from resnets_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7IauzQc-EgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the identity block of resnet without the skip connections\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEWUD0fL-RCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the convolution block without the skip connections\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "\n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "193qP-dVJn5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating ResNet-50\n",
        "\n",
        "def ResNet50(input_shape = (40, 40, 1)):\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n",
        "\n",
        "    # Output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(1, activation='sigmoid', name='fca' + str(1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxKOFG1qAAh8",
        "colab_type": "code",
        "outputId": "b199f42c-1b70-4638-85b5-0aeb0c513841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "#Training the architecture on our data\n",
        "from keras import backend as K \n",
        "import gc\n",
        "\n",
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "model = ResNet50(input_shape = input_shape)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn2 = model.fit(X_train, y_train,batch_size=256, epochs=20, verbose=1, validation_split=0.2)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 17636 samples, validate on 4410 samples\n",
            "Epoch 1/20\n",
            "17636/17636 [==============================] - 38s 2ms/step - loss: 0.6775 - accuracy: 0.7178 - val_loss: 1.4503 - val_accuracy: 0.5075\n",
            "Epoch 2/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.2921 - accuracy: 0.8732 - val_loss: 1.3764 - val_accuracy: 0.5075\n",
            "Epoch 3/20\n",
            "17636/17636 [==============================] - 19s 1ms/step - loss: 0.2016 - accuracy: 0.9205 - val_loss: 2.4304 - val_accuracy: 0.5075\n",
            "Epoch 4/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.1828 - accuracy: 0.9304 - val_loss: 0.7834 - val_accuracy: 0.5075\n",
            "Epoch 5/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.1556 - accuracy: 0.9402 - val_loss: 2.3212 - val_accuracy: 0.5075\n",
            "Epoch 6/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.1237 - accuracy: 0.9534 - val_loss: 1.9552 - val_accuracy: 0.5075\n",
            "Epoch 7/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.1052 - accuracy: 0.9599 - val_loss: 1.5814 - val_accuracy: 0.5177\n",
            "Epoch 8/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.1026 - accuracy: 0.9608 - val_loss: 3.3748 - val_accuracy: 0.5150\n",
            "Epoch 9/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0941 - accuracy: 0.9652 - val_loss: 0.5362 - val_accuracy: 0.8179\n",
            "Epoch 10/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0798 - accuracy: 0.9706 - val_loss: 1.0575 - val_accuracy: 0.6939\n",
            "Epoch 11/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0660 - accuracy: 0.9758 - val_loss: 0.6513 - val_accuracy: 0.7986\n",
            "Epoch 12/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0659 - accuracy: 0.9747 - val_loss: 8.2811 - val_accuracy: 0.5077\n",
            "Epoch 13/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0644 - accuracy: 0.9766 - val_loss: 0.4816 - val_accuracy: 0.8893\n",
            "Epoch 14/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0470 - accuracy: 0.9823 - val_loss: 0.7931 - val_accuracy: 0.8596\n",
            "Epoch 15/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.3980 - val_accuracy: 0.8782\n",
            "Epoch 16/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0480 - accuracy: 0.9827 - val_loss: 4.0508 - val_accuracy: 0.5717\n",
            "Epoch 17/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0559 - accuracy: 0.9803 - val_loss: 0.9355 - val_accuracy: 0.7229\n",
            "Epoch 18/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0473 - accuracy: 0.9840 - val_loss: 13.3048 - val_accuracy: 0.5079\n",
            "Epoch 19/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0341 - accuracy: 0.9878 - val_loss: 0.7505 - val_accuracy: 0.8270\n",
            "Epoch 20/20\n",
            "17636/17636 [==============================] - 20s 1ms/step - loss: 0.0393 - accuracy: 0.9858 - val_loss: 15.1625 - val_accuracy: 0.5075\n",
            "5512/5512 [==============================] - 3s 554us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15.783321013817421, 0.49111029505729675]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwMfv_iY96E",
        "colab_type": "text"
      },
      "source": [
        "We can see that the loss of the model is high, and the test accuracy is low, and the validation accuracy is fluctuating without the skip connections.Let us try adding the skip connections. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_ZtjVmb4KGd",
        "colab_type": "text"
      },
      "source": [
        "### Adding Residual connections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFhejMJ6P1V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del identity_block\n",
        "del convolutional_block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6zNBBClbKz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the skip connection in the identity block\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIacmC4O2ZXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding skip connection in the Convolutional Block\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "\n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyZILQzu2sKA",
        "colab_type": "code",
        "outputId": "a2485c64-d9c2-45b7-de19-5acadfe71676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "#Training the architecture on our data\n",
        "\n",
        "model = ResNet50(input_shape = input_shape)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn3 = model.fit(X_train, y_train,batch_size=256, epochs=20, verbose=1)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "22046/22046 [==============================] - 35s 2ms/step - loss: 0.6352 - accuracy: 0.7547\n",
            "Epoch 2/20\n",
            "22046/22046 [==============================] - 24s 1ms/step - loss: 0.3554 - accuracy: 0.8616\n",
            "Epoch 3/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.2361 - accuracy: 0.9101\n",
            "Epoch 4/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1803 - accuracy: 0.9293\n",
            "Epoch 5/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1446 - accuracy: 0.9445\n",
            "Epoch 6/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1253 - accuracy: 0.9499\n",
            "Epoch 7/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1157 - accuracy: 0.9541\n",
            "Epoch 8/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1073 - accuracy: 0.9586\n",
            "Epoch 9/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.1011 - accuracy: 0.9607\n",
            "Epoch 10/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0853 - accuracy: 0.9666\n",
            "Epoch 11/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0715 - accuracy: 0.9725\n",
            "Epoch 12/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0829 - accuracy: 0.9697\n",
            "Epoch 13/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0882 - accuracy: 0.9661\n",
            "Epoch 14/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0610 - accuracy: 0.9769\n",
            "Epoch 15/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0552 - accuracy: 0.9798\n",
            "Epoch 16/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0721 - accuracy: 0.9727\n",
            "Epoch 17/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0530 - accuracy: 0.9801\n",
            "Epoch 18/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0488 - accuracy: 0.9822\n",
            "Epoch 19/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0475 - accuracy: 0.9835\n",
            "Epoch 20/20\n",
            "22046/22046 [==============================] - 23s 1ms/step - loss: 0.0869 - accuracy: 0.9690\n",
            "5512/5512 [==============================] - 4s 783us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23443536100306256, 0.9272496104240417]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mztP7wJoSRad",
        "colab_type": "text"
      },
      "source": [
        "We have seen that adding the skip connections for the ResNet-50 architecture has helped the model learn. So we can conclude that for deeper networks adding skip connections helps."
      ]
    }
  ]
}